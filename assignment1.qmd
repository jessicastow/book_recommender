---
title: "Book Recommendation System for Young Adults"
author: "Jessica Stow (STWJES003@MYUCT.AC.ZA)"
date: "25 September 2024"
output: 
  html_document:
    fig_width: 8
    fig_height: 4
    code_folding: true
  pdf_document:
    fig_width: 8
    fig_height: 4
---

# View this report on my GitHub profile!

This report's repository can be viewed on [my GitHub profile](https://github.com/jessicastow/book_recommender).

# Plagiarism declaration

-   I know that plagiarism is wrong.

-   Plagiarism is to use another’s work and pretend that it is one’s own.

-   I have used the required convention for citation and referencing.

-   Each contribution to and quotation in this assignment from the work(s) of other people has been attributed, and has been cited and referenced.

-   This assignment is my own work.

-   I have not allowed, and will not allow, anyone to copy my work with the intention of passing it off as his or her own work.

-   I acknowledge that copying someone else’s assignment or essay, or part of it, is wrong, and declare that this is my own work.

# Objectives

The objectives of this report were as follows:

1\. Build recommender systems that predict the rating a user will give to a book based on each of:

a\) item-based collaborative filtering,

b\) user-based collaborative filtering, and

c\) matrix factorisation.

2\. Assessment and ensemble model:

a\) Assess the accuracy of the matrix factorisation recommender system, using a single train/test sample.

b\) Assess the accuracy of the matrix factorisation recommender system with and without regularisation.

c\) Create a final model that ensembles the predictions from the three approaches, and then assess the accuracy of the ensemble predictions.

# Introduction to Data Mining and Recommender Systems

Recommender systems utilise data mining techniques to offer personalised suggestions by analysing patterns in user preferences and behaviours (Data Mining: Concepts and Techniques, 2012). The collaborative filtering approach specifically focuses on identifying users with similar tastes or preferences, recommending items based on the opinions and actions of those with shared interests. This method may also take into account a user’s social environment to enhance the relevance of the recommendations.

## Cosine Similarity

Cosine similarity is a metric used in both user-based and item-based collaborative filtering to measure how similar two vectors are.

Given two vectors, $\boldsymbol x$ and $\boldsymbol y$, the cosine similarity is defined as:

$$cos(\theta) = \frac{\boldsymbol x \cdot \boldsymbol y}{||\boldsymbol x|| \ ||\boldsymbol y||} = \frac{\sum_{i=1}^{n}x_iy_i}{\sqrt{\sum_{i=1}^{n}x^2_i} \sqrt{\sum_{i=1}^{n}y^2_i}}$$

Cosine similarity ranges from 0 to 1, with higher values indicating greater similarity. As two vectors become more aligned, the angle between them decreases, and the cosine similarity approaches 1, reflecting highly similar user preferences. Conversely, a larger angle results in a cosine similarity closer to 0, indicating that the preferences are very different.

## User-Based Collaborative Filtering

User-Based Collaborative Filtering accounts for a user's interests by identifying similar users and recommending items that those users have shown interest in (Grus, 2015).

To get an idea of how similar two users are we need to use the cosine similarity metric, which quantifies how alike any two users are based on their preference vectors.

## Item-Based Collaborative Filtering

Item-based collaborative filtering (IBCF) takes an alternative approach by computing similarities between items, rather than users. Recommendations are then generated for each user by aggregating items that are similar to the ones the user has shown interest in (Grus, 2015).

Cosine similarity is again used to calculate similarity. If two items are of interest to the same users, their similarity will be closer to 1. If no users show interest in both items, their similarity will be closer to 0. Recommendations are generated by summing the similarities of items related to the user's current interests.

## Collaborative filtering with matrix factorisation

Matrix factorisation offers a different approach to collaborative filtering, rooted in linear algebra, where the goal is to fill in missing values within a matrix. Also known as matrix decomposition, it involves representing a matrix as the product of two smaller matrices. The key concept behind this method is the discovery of **latent factors** — hidden features that capture meaningful patterns in the data.

In recommendation systems, matrix factorisation decomposes the user-item ratings matrix into two smaller matrices in such a way that the known ratings are closely approximated. A key advantage of this approach is that, while the original ratings matrix is incomplete (with missing entries), the decomposed matrices are fully populated. This allows for predicting the missing values in the original matrix, effectively filling in the blanks and making recommendations based on the latent factors derived from the data.

# Data description

The dataset used in this report was obtained from [Kaggle's freely available Book Recommendation Dataset](https://www.kaggle.com/datasets/arashnic/book-recommendation-dataset/). The data was collected by Cai-Nicolas Ziegler in a four-week-long crawl between August and September 2004 from the [Book-Crossing community](https://www.bookcrossing.com/) with kind permission from Ron Hornbaker, CTO of Humankind Systems. It contains 278 858 users (anonymised, but with demographic information) providing over 1 million ratings (explicit/implicit) about 271 379 books.

The dataset consists of three files:

1\. **Users:** which contains the user information:

`User.ID`: the unique, anonymised user identifier.

`Location`: the location of the user (in the format of city, state, country).

`Age`: the age of the user.

2\. **Books:** which contains the book and content based information (which have been obtained from Amazon Web Services):

`ISBN`: the unique identifier for each book.

`Book.Title`: the book title.

`Book.Author`: the book author (in the case of several authors, only the first is provided).

`Year.Of.Publication`: the year of publication.

`Publisher`: the publisher of the book.

`Image-URL-S`, `Image-URL-M`, and `Image-URL-L`: URLs linking to cover images of the books in size small, medium and large, respectively. These URLs point to the Amazon web site.

3\. **Ratings:** which contains the book rating information:

`User.ID`: the unique user id of the user rating the book.

`ISBN`: the ISBN (identifier) of the book rated.

`Book.Rating`: the rating given by the user. These ratings are either explicit (expressed on a scale of 1-10 where higher values indicated higher appreciation), or implicit, expressed by 0.

# Exploratory data analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      error = FALSE,
                      include = FALSE,
                      warning = FALSE, 
                      cache = TRUE)
```

```{r}
# Load libraries
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(DT)
library(keras)
library(recosystem)
library(Matrix)
library(proxyC)
```

There were no duplicate entries found in any of the datasets.

The `Books` dataset contains information for 271 360 books, with all of these books being unique based on their ISBN.

The `Users` dataset contains information for 278 858 users, with all of these users being unique based on their user IDs.

The `Ratings` dataset contains information for 1 149 780 ratings. For this dataset we found that 105 283 unique users gave ratings (both explicit and implicit), this accounts for approximately 38% of the total user base. Interestingly, these users rated 340 556 books, which exceeds the number of books listed in the `Books` dataset.

```{r}
# read in data
books <- read.csv("data/Books.csv")
ratings <- read.csv("data/Ratings.csv")
users <- read.csv('data/Users.csv')

# BOOKS -----------------------------------------------------------------------
# are all books unique?
nrow(books) # 271 360 books
length(unique(books$ISBN)) # 271 360 books 
nrow(books) == length(unique(books$ISBN)) # YES
sum(duplicated(books)) # no duplicates as expected

# USERS -----------------------------------------------------------------------
# are all users unique?
nrow(users) # 278 858 users
length(unique(users$User.ID)) # 278 858 users
nrow(users) == length(unique(users$User.ID)) # YES
sum(duplicated(users)) # no duplicates as expected



# RATINGS ----------------------------------------------------------------------
# how many ratings?
nrow(ratings) # 1 149 780 ratings
sum(duplicated(ratings)) # no duplicates
# how many users gave ratings?
length(unique(ratings$User.ID)) # 105 283

# what percentage of users gave ratings?
round(length(unique(ratings$User.ID))/length(unique(users$User.ID))*100) # ~38%

# how many books were rated?
length(unique(ratings$ISBN)) # 340 556
```

## Univariate analysis

We decided to investigate the following variables: user age and book ratings.

We chose not to explore user location or additional book details, as they were not relevant to the objectives of this recommender system exercise.

### User Age

User ages ranged from 0 to 244 years, with a mean of 35 and a median of 32. This range presents significant issues, as it's unrealistic for users to be 0 or 244 years old. These outliers likely result from data entry errors. Excluding these extreme values reveals that the age distribution is generally symmetrical and uniform.

```{r include=TRUE}
# USERS -----------------------------------------------------------------------
# what is the spread of user ages?
min(users$Age, na.rm = TRUE) # minimum age = 0 
mean(users$Age, na.rm = TRUE) # mean age 35 years
median(users$Age, na.rm = TRUE) # 32 
max(users$Age, na.rm = TRUE) # max age 244

# Visualise the range of 'Age' using a boxplot
boxplot(users$Age, 
        main = "Figure 1: Boxplot of User Age", 
        ylab = "Age", 
        col = "lightgreen", 
        horizontal = TRUE)

```

### Book ratings

Book ratings ranged from 0 to 10. A rating of 0 indicated an implicit interaction, meaning the user may have read or interacted with the book but did not explicitly rate it. Ratings from 1 to 10 were explicit, representing direct user input. As shown in *Figure 2* below, the majority of ratings were implicit, which will be less useful for our recommender system moving forward.

```{r include=TRUE}
# RATINGS -----------------------------------------------------------------------
# what is the spread of the ratings variable?
min(ratings$Book.Rating, na.rm = TRUE) # minimum rating = 0 
mean(ratings$Book.Rating, na.rm = TRUE) # mean age 35 years
median(ratings$Book.Rating, na.rm = TRUE) # 32 
max(ratings$Book.Rating, na.rm = TRUE) # max rating = 10

# Visualise the range of 'rating' using a histogram
hist(ratings$Book.Rating, 
     breaks = 11, 
     main = "Figure 2: Distribution of Book rating", 
     xlab = "Book rating",
     col = "lightblue", 
     border = "black",
     xaxt = "n")

# Add custom x-axis with labels from 0 to 10
axis(1, at = 0:10, labels = 0:10)
```

## Bivariate analysis

### Ratings by user

We observed that one user (ID 11676) rated an impressive 13 607 books, which is significantly higher than the average of just 10 book ratings per user. When we investigated this user we noted that there is no location or age information available for them

```{r}
# which users rated the most books?

# Group by User ID and count the number of ratings
user_ratings_count <- ratings %>%
  group_by(User.ID) %>%
  summarise(rating_count = n()) %>%
  arrange(desc(rating_count))

# View the top user who rated the most books
head(user_ratings_count, 1)

# investigate this user
users[users$User.ID == '11676', ]

# average ratings per user
mean(user_ratings_count$rating_count)
```

### Ratings by book

The most rated book was "Wild Animus" by Rich Shapero, published in 2004, which received a total of 2502 ratings.

```{r}
# which book got the most ratings?

# Group by ISBN and count the number of ratings for each book
book_ratings_count <- ratings %>%
  group_by(ISBN) %>%
  summarise(rating_count = n()) %>%
  arrange(desc(rating_count))

# View the book that received the most ratings
head(book_ratings_count, 1)

# which book was this?
books[books$ISBN == '0971880107', ]
```

Several books had an average rating of 10, though in many cases these ratings were based on input from only a single user.

```{r}
# which book got the highest rating?

# Group by ISBN and calculate the mean rating for each book
book_avg_ratings <- ratings %>%
  group_by(ISBN) %>%
  summarise(average_rating = mean(Book.Rating, na.rm = TRUE), 
            rating_count = n()) %>%
  arrange(desc(average_rating))

# View the top 10 books with the highest average ratings
head(book_avg_ratings, 10)

```

# Methods

## Data manipulation

### Ratings

We decided to drop the implicit ratings (ratings equal to 0) as these are not useful in the building of our recommender model. We also chose to only keep books with 5 or more ratings to exclude books that are rarely read.

```{r}
ratings <- ratings %>%
  filter(Book.Rating != 0) %>% # Book ratings of zero imply the user did not rate the book, drop rows where rating = 0 
  group_by(ISBN) %>%
  filter(n() > 4) %>% # only keep books with 5 or more ratings
  ungroup()

nrow(ratings) # 199 477 rows, therefore 950 303 rows (83% of the data) dropped
```

We also chose to retain user ratings only for those who had rated 10 or more books (i.e., users who rated at least the average number of books). This approach ensures that we focus on active readers, providing a more reliable understanding of their preferences. These filtering methods resulted in a ratings data frame with 111 008 rows, meaning 1 038 772 observations (\~90% of our ratings data) was dropped.

```{r}
# only keep ratings when users have rated 10 or more books
ratings <- ratings %>%
  group_by(User.ID) %>%
  filter(n() >= 10) %>%
  ungroup()
```

### Users

We decided to only investigate the user preferences of young adults between the ages of 18 and 25 (inclusive). All other ages were dropped from the data frame. We also decided to exclude the user location information as this was not helpful to our recommender model building.

```{r}
users <- users %>%
  filter(Age >= 18 & Age <= 25) %>% # only keep users between 18 and 25 years old
  select(-Location)
```

### Books

We ensured that all books had a title. We then investigated the instances where the Year of Publication contained non-numeric values, as this would prevent converting the column to an integer. We found that for three books, the Book Author and Year of Publication fields had been mistakenly swapped. To address this, we implemented a solution to correct the swapped information. We then turned the year of publication column into a type integer.

We kept the ISBN, book title, author and year of publication information and dropped the publisher and image URLs from the `books` data frame.

```{r}
sum(is.na(books$Book.Title)) # all books have a title

# identify the cases where the year is not numeric
non_numeric_year <- books[is.na(as.numeric(as.character(books$Year.Of.Publication))), ]

# Swap the 'Book.Author' and 'Year.Of.Publication' values for these rows
books$Year.Of.Publication[is.na(as.numeric(as.character(books$Year.Of.Publication)))] <- non_numeric_year$Book.Author
books$Book.Author[is.na(as.numeric(as.character(books$Year.Of.Publication)))] <- non_numeric_year$Year.Of.Publication

colnames(books)
books <- books %>%  
  mutate(Year.Of.Publication = as.integer(Year.Of.Publication)) %>% # make year of publication an integer
  select(c(ISBN, Book.Title, Book.Author, Year.Of.Publication, # keep these variables
           -Publisher, -Image.URL.S, -Image.URL.M, -Image.URL.L)) # drop these variables
```

### Joining of data frames

The `Books` data frame was merged with the `Ratings` data frame using a left join on the common variable`ISBN`. This ensured that all the ratings information was retained while adding the corresponding book titles and authors to the data set.

To address the earlier discrepancy where more books were rated than were listed in the `Books` data frame, we checked for any missing book titles. We found that 7 249 ratings lacked corresponding book titles or any other associated information. As a result, we opted to remove these ratings from the data frame.

```{r}
# left join books to ratings df and rename it "ratings_books"
ratings_books <- ratings %>%
  left_join(books, by = "ISBN") %>%
  arrange(User.ID) # order by user ID

sum(is.na(ratings_books$Book.Title)) # 7249 ratings without book info

# Remove rows where Book.Title is NA
ratings_books <- ratings_books %>% 
  filter(!is.na(Book.Title))
```

We then merged the user information into the new data frame using a left join, matching on the shared user IDs. The left join was chosen to ensure that all data related to the books and their ratings remained intact. We then noted that we now had 97 202 ratings without age information which had to be dropped.

```{r}
ratings_books_users <- ratings_books %>%
  left_join(users, by = "User.ID")

sum(is.na(ratings_books_users$Age)) # 97202 ratings without age info

# Remove rows where Age is NA
ratings_books_users <- ratings_books_users %>% 
  filter(!is.na(Age))
```

The final merged data frame contained the following variables: the book's ISBN, title, author, year of publication, the user's rating, and their age. It comprised 10 768 ratings, and the first 50 can be viewed by exploring the data table below.

```{r}
nrow(ratings_books_users) # 10768
colnames(ratings_books_users)
```

```{r include=TRUE}
datatable(
  head(ratings_books_users, 50), 
  rownames = FALSE,
  options = list(dom = "tip", pageLength = 5)
)
```

## Collaborative filtering with matrix factorisation

Matrix factorisation was implemented using the `recosystem` package. To prepare the data in the required format, we structured the ratings dataset so that each row represented a unique rating, with columns for the user ID, book ISBN, book title, and rating score. It was necessary to convert both the user IDs and book ISBNs into factors, as the matrix factorisation model in `recosystem` relies on categorical encoding of users and items. This step ensured that the model could properly identify and differentiate between users and books.

```{r}
# in order to do matrix factorisation we need to make the user IDs and ISBNs factors
mf <- ratings_books_users %>%
  select(User.ID, ISBN, Book.Title, Book.Rating) %>%
  mutate(User.ID = as.factor(User.ID)) %>%
  mutate(ISBN = as.factor(ISBN)) 
```

After preparing the data, we initialised a matrix factorisation model using the `recosystem` package by creating an instance of the `Reco()` object. This object serves as the model that will be trained on our data to learn user-item interactions.

Next, we split the data into training and test sets to evaluate the model’s performance. We used an 80/20 split, where 80% of the data was randomly selected for training and the remaining 20% was used for testing. To ensure reproducibility so that the same random split can be generated each time the code is run, we set a random seed.

```{r}
# Initialise the recosystem object
r <- Reco()

# Create train and test sets 
set.seed(123)  # For reproducibility
train_indices <- sample(nrow(mf), size = 0.8 * nrow(mf))
train_set <- mf[train_indices, ]
test_set <- mf[-train_indices, ]
```

To use the `recosystem` package for matrix factorisation, we need to convert the training and testing data into a format that the package can process. This is done by using the `data_memory()` function, which transforms the data into memory-efficient objects that `recosystem` can work with.

**For the training data:**

-   We specify the `user_index` (the user ID), `item_index` (the ISBN for each book), and the rating (the actual rating given by the user).

-   This creates a memory-based data object that links users, books, and ratings, which will be used to train the model.

**For the testing data:**

-   A similar process was applied, but with the test set. The `user_index` and `item_index` were specified along with the rating, allowing for evaluation of the model's performance on unseen data.

-   The training set comprised 8,614 observations, which were utilised to train the model. 

```{r}
# Prepare the training and testing data for recosystem

# training data 
train_data <- data_memory(
  user_index = train_set$User.ID, 
  item_index = train_set$ISBN, 
  rating = train_set$Book.Rating
)

# testing data
test_data <- data_memory(
  user_index = test_set$User.ID, 
  item_index = test_set$ISBN, 
  rating = test_set$Book.Rating
)
```

Collaborative filtering was subsequently conducted using matrix factorisation, both with and without the inclusion of a regularisation term.

### Without regularisation term

The matrix factorisation model was trained on the training set using the `train()` function from the `recosystem` package. In the absence of a regularisation term, the model exclusively learned latent factors for users and items without penalising large or complex factor values. This approach increased the risk of overfitting, particularly with sparse data, as the model could fit the training data too closely, capturing noise rather than identifying generalisable patterns.

After completing the training phase, predictions were generated for the test set using the `predict()` function. These predicted ratings were stored for later evaluation. However, due to the lack of regularisation, while the model might have performed well on the training set, it may have struggled to generalise to unseen data in the test set, potentially resulting in reduced predictive accuracy.

```{r}
# Train the model on the training set
r$train(train_data)

# Predict on the test set
test_set$MF_pred <- r$predict(test_data)
```

### With regularisation term

To improve the performance of matrix factorisation in collaborative filtering, L2 regularisation was introduced. L2 regularisation incorporates a penalty term into the loss function, which discourages large parameter values. This technique helps mitigate overfitting, particularly in sparse datasets, a common issue in recommendation systems. In the `recosystem` package, L2 regularisation is applied to both user and item latent factors by adjusting the `costp_l2` and `costq_l2` parameters during model training. These parameters regulate the strength of regularisation for the user and item factors, respectively.

A grid search was conducted to determine the optimal combination of `costp_l2` and `costq_l2` values that would minimise the RMSE for the predictions.

The following values were tested:

- `costp_l2`: 0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1
- `costq_l2`: 0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1

This method enabled the identification of the most effective regularisation settings to enhance prediction accuracy and prevent overfitting.

```{r}
# Define the grid search parameters
costp_l2_values <- c(0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1)
costq_l2_values <- c(0.001, 0.01, 0.1, 0.5, 0.6, 0.8, 1)

# Create a dataframe to store the results
results <- data.frame(costp_l2 = numeric(),
                      costq_l2 = numeric(),
                      rmse = numeric())

# Loop over the different combinations of costp_l2 and costq_l2
for (costp_l2 in costp_l2_values) {
  for (costq_l2 in costq_l2_values) {
    # Train the model with the current combination of hyperparameters
    r$train(train_data, opts = list(costp_l2 = costp_l2, costq_l2 = costq_l2, niter = 20))
    
    # Predict on the test set
    test_set$MF_pred <- r$predict(test_data)
    
    # Calculate the Root Mean Square Error (RMSE)
    rmse <- sqrt(mean((test_set$MF_pred - test_set$Book.Rating)^2, na.rm = TRUE))
    
    # Store the results
    results <- rbind(results, data.frame(costp_l2 = costp_l2, costq_l2 = costq_l2, rmse = rmse))
    
    # Print the current combination and its RMSE
    print(paste("costp_l2:", costp_l2, "costq_l2:", costq_l2, "RMSE:", rmse))
  }
}
```

The best combination of hyperparameters were a `costp_l2` of 0.5 and a `costq_l2` of 0.01, which resulted in a RMSE of ~1.83. 

```{r}
# Find the best hyperparameters
best_params <- results[which.min(results$rmse), ]
print(best_params)
```

This combination was selected for the final model with L2 regularisation to generate predictions.

```{r}
r$train(train_data, opts = list(
  costp_l2 = 0.5,  # Regularisation for user factors
  costq_l2 = 0.01))  # Regularisation for item factors

# Predict on the test set
test_set$MF_pred_L2 <- r$predict(test_data)
```

## User-Based Collaborative Filtering

To compute user similarities, the ratings matrix was first converted into a wide format, where each row corresponded to a user (User.ID) and each column represented a book (ISBN).

Given that users exhibit different rating tendencies, with some consistently assigning higher or lower ratings, mean-centering was applied to standardise the data. Without this normalisation, users with similar preferences could appear dissimilar due to variations in their rating styles. Mean-centering adjusted the ratings to reflect how much a user liked or disliked a book **relative to their own average**, thereby aligning their rating patterns. This step ensured the data was appropriately prepared for calculating cosine similarity between users, capturing their true preferences and facilitating more accurate recommendations.

```{r}
# transform ratings matrix into wide format 
ratings_wide <- ratings_books_users %>%
   select(User.ID, ISBN, Book.Rating) %>%
   complete(User.ID, ISBN) %>%
   pivot_wider(names_from = ISBN, values_from = Book.Rating)

# extract user names to use for row names in ratings_wide matrix
sorted_my_users <- as.character(unlist(ratings_wide[, 1]))

# Compute the mean rating for each user
user_means <- rowMeans(ratings_wide[,-1], na.rm = TRUE)

# create a user means dictionary
user_means_dict <- setNames(user_means, sorted_my_users)

# Subtract the mean rating from each user's ratings (mean-centering)
ratings_wide <- sweep(ratings_wide[,-1], 1, user_means, FUN = "-")

# convert to matrix 
ratings_wide <- as.matrix(ratings_wide)

# assign user IDs to row names of matrix
row.names(ratings_wide) <- sorted_my_users
```

Any `NA` values, representing missing ratings for a given book, were replaced with zero. This step was essential, as cosine similarity cannot process missing values. Following this, the matrix was converted into a sparse format, which is more computationally efficient when dealing with matrices that contain a high proportion of zero values.

```{r}
# Replace NAs with 0, since cosine similarity doesn't handle NAs directly
ratings_wide[is.na(ratings_wide)] <- 0

# Convert the matrix to a sparse matrix
# sparse matrices are computationally faster when dealing with matrices with lots of 0s
ratings_wide_sparse <- Matrix(ratings_wide, sparse = TRUE)
```

Cosine similarities between users, based on their book ratings, were calculated using the `simil()` function from the `proxyC` package, which is optimised for efficiently handling sparse matrices. The resulting user similarities sparse matrix was then converted into a dense matrix to facilitate easier computation and predictions in subsequent steps.

```{r}
# Compute cosine similarity between users based on ratings given to books
# using proxyC which deals well with sparse matrices
user_similarities <- simil(ratings_wide_sparse, method = "cosine")

# Convert the result to a dense matrix
user_similarities <- as.matrix(user_similarities)

# View the first few rows and columns
user_similarities[1:5, 1:5]
```

### Predictions

To evaluate the predictions of the user-based collaborative filtering model, predictions were made on the same test set used in the matrix factorisation model. A function was created to calculate the user-based predicted rating while accounting for each user's average rating.

Within the function, the cosine similarity scores for the target user were first retrieved and then standardised so that they summed to one, but only across users who had rated the specific book. The predicted rating for the book was computed as a weighted sum of the ratings provided by other users, with the weights corresponding to the standardised similarity scores. To return the prediction to its original scale, the user's average rating was added to the result.

This approach ensured that both user similarities and individual rating patterns were accounted for when generating predictions.

```{r}
# Define a function to calculate the user-based predicted rating with user mean adjustment
ub_predict_rating <- function(user_id, isbn, ratings_wide, user_similarities, user_means_dict) {
  # Get the similarities for the current user
  stdW <- user_similarities[user_id, ] * !is.na(ratings_wide[, isbn]) # only for users that have rated that book
  
  # Normalise the similarities (to ensure they sum to 1)
  stdW <- stdW / sum(stdW, na.rm = TRUE)
  
  # Calculate the predicted rating using weighted sum
  ub_rating <- sum(ratings_wide[, isbn] * stdW, na.rm = TRUE) 
  
  # Add the user's mean rating to the predicted rating
  ub_rating_adjusted <- ub_rating + user_means_dict[user_id]
  
  return(ub_rating_adjusted)
}
```

The function was then applied to the same test set used in the collaborative filtering model with matrix factorisation.

```{r}
# Apply the function across the test set dataframe
test_set$UB_pred <- mapply(ub_predict_rating, 
                           test_set$User.ID, 
                           test_set$ISBN, 
                           MoreArgs = list(ratings_wide = ratings_wide, 
                                           user_similarities = user_similarities, 
                                           user_means_dict = user_means_dict))

```

## Item-Based Collaborative Filtering

To compute item similarities, the ratings matrix was first transformed into a wide format, where each row represented a book (ISBN) and each column represented a user (User.ID).

Given that books may receive varying ratings from different users, mean-centering was applied to standardise the data. Without this normalisation, books with similar levels of popularity could appear dissimilar due to differences in user rating habits. By applying mean-centering, the ratings were adjusted to reflect how well a book was rated relative to its average rating across all users. This step ensured that the data was properly prepared for calculating cosine similarity between items, capturing each book’s true popularity and enabling more accurate recommendations.

```{r}
# Transform ratings matrix into wide format, with books (ISBN) as rows and users (User.ID) as columns
books_ratings_wide <- ratings_books_users %>%
   select(User.ID, ISBN, Book.Rating) %>%
   complete(User.ID, ISBN) %>%
   pivot_wider(names_from = User.ID, values_from = Book.Rating)

# Extract book ISBNs to use for row names in books_ratings_wide matrix
sorted_my_books <- as.character(unlist(books_ratings_wide[, 1]))

# Compute the mean rating for each book
book_means <- rowMeans(books_ratings_wide[,-1], na.rm = TRUE)

# Create a book means dictionary
book_means_dict <- setNames(book_means, sorted_my_books)

# Subtract the mean rating from each book's ratings (mean-centering)
books_ratings_wide <- sweep(books_ratings_wide[,-1], 1, book_means, FUN = "-")

# Convert to matrix
books_ratings_wide <- as.matrix(books_ratings_wide)

# Assign book titles (ISBNs) to row names of the matrix
row.names(books_ratings_wide) <- sorted_my_books
```

Any `NA` values, representing missing ratings for a given user, were replaced with zero. This step was essential as cosine similarity cannot process missing values. The matrix was then transformed into a sparse format, improving computational efficiency when working with matrices containing a significant number of zero values.

```{r}
# Replace NAs with 0, since cosine similarity doesn't handle NAs directly
books_ratings_wide[is.na(books_ratings_wide)] <- 0

# Convert the matrix to a sparse matrix for computational efficiency
books_ratings_wide <- Matrix(books_ratings_wide, sparse = TRUE)
```

Cosine similarities between books, based on their ratings from users, were calculated using the `simil()` function from the `proxyC` package, which is optimised for efficiently handling sparse matrices. The resulting item similarities sparse matrix was then converted into a dense matrix to facilitate easier computation and predictions in subsequent steps.

```{r}
# Compute cosine similarity between books based on user ratings
# using proxyC which deals well with sparse matrices
item_similarities <- simil(books_ratings_wide, method = "cosine")

# Convert the result to a dense matrix
item_similarities <- as.matrix(item_similarities)

# View the first few rows and columns
item_similarities[1:5, 1:5]
```

### Predictions

To evaluate the item-based (IB) collaborative filtering model's predictions, the same test set used for the matrix factorisation model was employed. A function was developed to calculate the predicted rating for each user, while adjusting for the user's average rating.

The function first retrieved the cosine similarity scores for the target item and standardised them so that they summed to one, but only across items that had been rated by the specific user. The predicted rating for the item was then calculated as a weighted sum of the ratings provided for similar items, with the weights determined by the standardised similarity scores. Finally, the predicted rating was adjusted by adding the user's average rating to return it to the original scale.

This approach ensured that both item similarities and individual rating tendencies were accounted for when generating predictions.

```{r}
# Define a function to calculate the user-based predicted rating with user mean adjustment
ib_predict_rating <- function(user_id, isbn, books_ratings_wide, item_similarities, book_means_dict) {
  # Get the similarities for the current book
  stdW <- item_similarities[isbn, ] * !is.na(books_ratings_wide[, user_id]) # only for users that have rated that book
  
  # Normalise the similarities (to ensure they sum to 1)
  stdW <- stdW / sum(stdW, na.rm = TRUE)
  
  # Calculate the predicted rating using weighted sum
  ib_rating <- sum(books_ratings_wide[, user_id] * stdW, na.rm = TRUE) 
  
  # Add the book's mean rating to the predicted rating
  ib_rating_adjusted <- ib_rating + book_means_dict[isbn]
  
  return(ib_rating_adjusted)
}
```


The function was subsequently applied to the same test set used in the matrix factorisation-based collaborative filtering model.

```{r}
# Apply the function across the test set dataframe
test_set$IB_pred <- mapply(ib_predict_rating, 
                           test_set$User.ID, 
                           test_set$ISBN, 
                           MoreArgs = list(books_ratings_wide = books_ratings_wide, 
                                           item_similarities = item_similarities, 
                                           book_means_dict = book_means_dict))
```

## Ensemble model

The final model was an ensemble, which averaged the predictions from the user-based, item-based, and matrix factorisation models (without regularisation). Predictions were only included in the averaging process if they fell within the (valid) range of 1 to 10 (inclusive).

```{r}
# Define a function to compute the ensemble prediction
ensemble_mean <- function(mf, ub, ib) {
  # Filter predictions that are between 1 and 10
  valid_preds <- c(mf, ub, ib)[c(mf, ub, ib) >= 1 & c(mf, ub, ib) <= 10]
  
  # If no valid predictions, return NA or another default value (e.g., mean = 5)
  if (length(valid_preds) == 0) {
    return(NA)
  }
  
  # Return the mean of the valid predictions
  return(mean(valid_preds))
}

# Apply the function row-wise to test set
test_set$ensemble_pred <- mapply(ensemble_mean, 
                                 test_set$MF_pred, 
                                 test_set$UB_pred, 
                                 test_set$IB_pred)
```


# Results

## Accuracy assessment

### Collaborative filtering with matrix factorisation model - Without regularisation term

```{r}
# Assess CF MF model using MSE & RMSE
# Calculate MSE
mse_MF <- mean((test_set$Book.Rating - test_set$MF_pred)^2)
print(mse_MF) # 3.39176

# Calculate RMSE
rmse_MF <- sqrt(mean((test_set$Book.Rating - test_set$MF_pred)^2))
print(rmse_MF) # 1.841673
```

### Collaborative filtering with matrix factorisation model - With regularisation term

```{r}
# Assess CF MF model with L2 regularisation term, using MSE & RMSE
# Calculate MSE
mse_MF_L2 <- mean((test_set$Book.Rating - test_set$MF_pred_L2)^2)
print(mse_MF_L2) # 3.303717

# Calculate RMSE
rmse_MF_L2 <- sqrt(mean((test_set$Book.Rating - test_set$MF_pred_L2)^2))
print(rmse_MF_L2) # 1.817613
```

### User-Based Collaborative filtering

```{r}
# Assess UB model predictions using MSE and RMSE
# Calculate MSE
mse_UB <- mean((test_set$Book.Rating - test_set$UB_pred)^2)
print(mse_UB) # 11.75365

# Calculate RMSE
rmse_UB <- sqrt(mean((test_set$Book.Rating - test_set$UB_pred)^2))
print(rmse_UB) # 3.42836
```

### Item-Based Collaborative filtering

```{r}
# Assess IB model predictions using MSE and RMSE
# Calculate MSE
mse_IB <- mean((test_set$Book.Rating - test_set$IB_pred)^2)
print(mse_IB) # 3861.38

# Calculate RMSE
rmse_IB <- sqrt(mean((test_set$Book.Rating - test_set$IB_pred)^2))
print(rmse_IB) # 62.14
```

### Ensemble model

```{r}
# Assess accuracy of ensemble model using MSE & RMSE
# Calculate MSE
mse_ensemble <- mean((test_set$Book.Rating - test_set$ensemble_pred)^2)
print(mse_ensemble) # 1.41263

# Calculate RMSE
rmse_ensemble <- sqrt(mean((test_set$Book.Rating - test_set$ensemble_pred)^2))
print(rmse_ensemble) # 1.188541
```

# Discussion

# Recommendations

# Conclusion

# References

Grus, J. (2015). Data Science from Scratch: First Principles with Python. 1st ed. O'Reilly Media.

Han, J., Kamber, M. & Pei, J. (2012). Data Mining: Concepts and Techniques, Morgan Kaufmann Publishers, San Francisco, CA, USA.
